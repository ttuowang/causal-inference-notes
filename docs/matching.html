<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Matching | Causal Inference</title>
  <meta name="description" content="This is the notes of Causal Inference. Most of the materials come from STAT 992 instructed by Prof. Kang and from several textbooks in causal inference" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Matching | Causal Inference" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the notes of Causal Inference. Most of the materials come from STAT 992 instructed by Prof. Kang and from several textbooks in causal inference" />
  <meta name="github-repo" content="ttuowang/causal-inference-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Matching | Causal Inference" />
  
  <meta name="twitter:description" content="This is the notes of Causal Inference. Most of the materials come from STAT 992 instructed by Prof. Kang and from several textbooks in causal inference" />
  

<meta name="author" content="Tuo Wang" />


<meta name="date" content="2019-10-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="sensitivity.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Causal Inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="matching.html"><a href="matching.html"><i class="fa fa-check"></i><b>2</b> Matching</a><ul>
<li class="chapter" data-level="2.1" data-path="matching.html"><a href="matching.html#what-is-matching"><i class="fa fa-check"></i><b>2.1</b> What is matching?</a></li>
<li class="chapter" data-level="2.2" data-path="matching.html"><a href="matching.html#exact-matching"><i class="fa fa-check"></i><b>2.2</b> Exact Matching</a></li>
<li class="chapter" data-level="2.3" data-path="matching.html"><a href="matching.html#propensity-score-matching"><i class="fa fa-check"></i><b>2.3</b> Propensity Score Matching</a></li>
<li class="chapter" data-level="2.4" data-path="matching.html"><a href="matching.html#multivariate-caliper-matching"><i class="fa fa-check"></i><b>2.4</b> Multivariate Caliper Matching</a><ul>
<li class="chapter" data-level="2.4.1" data-path="matching.html"><a href="matching.html#a-mahalanobis-distances"><i class="fa fa-check"></i><b>2.4.1</b> (a) Mahalanobis Distances</a></li>
<li class="chapter" data-level="2.4.2" data-path="matching.html"><a href="matching.html#b-penalize-large-distance"><i class="fa fa-check"></i><b>2.4.2</b> (b) Penalize large distance</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="matching.html"><a href="matching.html#matching-with-multiple-controls"><i class="fa fa-check"></i><b>2.5</b> Matching with Multiple controls</a></li>
<li class="chapter" data-level="2.6" data-path="matching.html"><a href="matching.html#full-matching"><i class="fa fa-check"></i><b>2.6</b> Full Matching</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sensitivity.html"><a href="sensitivity.html"><i class="fa fa-check"></i><b>3</b> Sensitivity Analysis</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="matching" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Matching</h1>
<p>This lecture note will cover the following topics:</p>
<ul>
<li>Exact Matching</li>
<li>Propensity Score Matching</li>
<li>Multivariate Caliper Matching</li>
<li>Matching with Multiple Controls</li>
<li>Full Matching</li>
</ul>
<div id="what-is-matching" class="section level2">
<h2><span class="header-section-number">2.1</span> What is matching?</h2>
<p>In observational study, absent random assignment, treated and control individuals may differ in terms of covariates, so direct comparison of the outcomes of treated individuals and controls may compare individuals who are not comparable - that is, a direct comparison may be biased as an estimate of the effect caused by the treatment.</p>
<p>Pros:</p>
<ul>
<li>Simple and intuitive</li>
<li>Blinding to outcome info</li>
<li>Diagnostic for overlap is easy</li>
</ul>
<p>Cons:</p>
<ul>
<li>Theory is difficult</li>
<li>It requires a lot of practice.</li>
</ul>
</div>
<div id="exact-matching" class="section level2">
<h2><span class="header-section-number">2.2</span> Exact Matching</h2>
<p><strong>Idea:</strong> Pair people with identical <span class="math inline">\(X\)</span>s</p>
</div>
<div id="propensity-score-matching" class="section level2">
<h2><span class="header-section-number">2.3</span> Propensity Score Matching</h2>
<p><strong>Quick Review on Propensity Score:</strong>
Propensity score is the conditional probability of exposure to treatment given the observed covariates, <span class="math inline">\(e(\mathbf{x}) = \mathbb{Pr}(A = 1|\mathbf{x})\)</span>, where <span class="math inline">\(\mathbf{x}\)</span> is the covaraites and <span class="math inline">\(A\)</span> is the intervention variable. The propensity score is unkown, but it can be estimated from the data. For example, we can use the logistic regression:</p>
<p><span class="math display">\[\log\{\frac{e(\mathbf{x_i})}{1 - e(\mathbf{x_i})}\} = \beta_0 + \mathbf{x_i}^T\bf{\beta}\]</span></p>
<p><strong>Idea:</strong> Pair people with similar <span class="math inline">\(e(X)\)</span>. We know <span class="math inline">\(X \perp A | e(X)\)</span>.</p>
<p>Calculate the propensity score for all subjects. Define the distance between subject i in the treatment group and subject j in the control group as:</p>
<p><span class="math display">\[d(\mathbf{x_i}, \mathbf{x_j}) = \big|\text{logit}(e(\mathbf{x_i})) - \text{logit}(e(\mathbf{x_i}))\big|\]</span></p>
</div>
<div id="multivariate-caliper-matching" class="section level2">
<h2><span class="header-section-number">2.4</span> Multivariate Caliper Matching</h2>
<p><strong>Why we need multivariate caliper matching?</strong></p>
<p>In lecture 6, we learned matching with propensity score, which tends to balance all of the covariates used to build that score, but two individuals with the same propensity score may differ in important ways. Also, propensity score matching is a single covariate matching, which ignore the interaction between different covarites.</p>
<div id="a-mahalanobis-distances" class="section level3">
<h3><span class="header-section-number">2.4.1</span> (a) Mahalanobis Distances</h3>
<p>Let <span class="math inline">\(\mathbf{x}\)</span> be the covariates random vector. Let <span class="math inline">\(\hat{\Sigma}\)</span> be the sample covariance matrix of <span class="math inline">\(\mathbf{x}\)</span>. Then the estimated Mahalanobis distance between subject <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, who has covarates <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(\mathbf{x_j}\)</span> repectively, is defined as:</p>
<p><span class="math display">\[d(\mathbf{x_i}, \mathbf{x_j}) = (\mathbf{x_i} - \mathbf{x_j})^T\hat{\Sigma}^{-1}(\mathbf{x_i} - \mathbf{x_j})\]</span></p>
<p>See the following comments on Mahalanobis distance from Rosenbaum’s book, Design of Observational Studies:</p>
<blockquote>
<p>“The Mahalanobis distance was originally developed for use with multivariate Nomal data, and for data of that type it works fine. With data that are not Normal, the Mahalanobis distance can exhibit some rather odd behavior. If one covariate contains extreme outliers of has a long-tailed distribution, its standard deviation will be inflated, and the Mahalanobis distance will tend to ignore that covariate in matching.”</p>
</blockquote>
<p>A simple alternative to the Mahalanobis distance is the ‘rank-based Mahalanobis distance’ which replaces each of the covariates by its ranks. Check out Design of Observational Studies, Chapter 8.3 for details.</p>
</div>
<div id="b-penalize-large-distance" class="section level3">
<h3><span class="header-section-number">2.4.2</span> (b) Penalize large distance</h3>
<p>The idea is that two individuals can be close on the propensity score to a degree, once this degree is achieved, covariates <span class="math inline">\(\mathbf{x}\)</span> may affect the distance. Define <span class="math inline">\(w\)</span> as the caliper width. With <span class="math inline">\(w\)</span>, if two individuals have propensity scores that differ more than <span class="math inline">\(w\)</span>, we will add a penalty to the ahalanobis distance between subject <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. Explicitly,</p>
<p><span class="math display">\[  d_{new}(\mathbf{x_i}, \mathbf{x_j})= 
\begin{cases}
    d(\mathbf{x_i}, \mathbf{x_j}) + p\times \big|\text{logit}(e(\mathbf{x_i}))-\text{logit}(e(\mathbf{x_j}))\big|,&amp; \text{if } \big|\text{logit}(e(\mathbf{x_i}))-\text{logit}(e(\mathbf{x_j}))\big| \geq w\\
    d(\mathbf{x_i}, \mathbf{x_j}),              &amp; \text{if } \big|\text{logit}(e(\mathbf{x_i}))-\text{logit}(e(\mathbf{x_j}))\big| &lt; w
\end{cases}\]</span></p>
<p>where <span class="math inline">\(e(\mathbf{x})\)</span> is the propensity score of covariates <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(d(\mathbf{x_i}, \mathbf{x_j})\)</span> is the Mahalanobis distance between individual <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> and <span class="math inline">\(p\)</span> is the penalty term.</p>
<p>In practical, we may want to care about the following things:</p>
<p><strong>What’s the value of <span class="math inline">\(p\)</span>?</strong></p>
<p>Usually <span class="math inline">\(p = 1000\)</span>. In <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-031219-041058">Paul Rosenbaum’s 2019 review on matching</a>: he used <span class="math inline">\(d_{new}(\mathbf{x_i}, \mathbf{x_j}) = +\infty\)</span> when <span class="math inline">\(\big|\text{logit}(e(\mathbf{x_i}))-\text{logit}(e(\mathbf{x_j}))\big| \geq w\)</span>.</p>
<p><strong>What’s the value of <span class="math inline">\(w\)</span>?</strong></p>
<p>People use <span class="math inline">\(w = 0.5 \times SD(\text{logit}(e(\mathbf{x})))\)</span> or <span class="math inline">\(w = 0.2 \times SD(\text{logit}(e(\mathbf{x})))\)</span> in practice.</p>
<p><strong>Comments:</strong> Use of the Mahalanobis distance inside propensity score calipers will balance covariates and also pair similar individuals. Also, using both Mahalanobis distance and propensity score calipers adds a protection against the failure of a single matching technique.</p>
</div>
</div>
<div id="matching-with-multiple-controls" class="section level2">
<h2><span class="header-section-number">2.5</span> Matching with Multiple controls</h2>
<p>In previous matching algorithms, we only match one treatment with one control. In matching with multiple controls, each treatment is matched to at least one control. For example if we match 1 treatment with 2 controls, assume we have 10 subjects in treatment group, we will end up with 10 matched sets, which each contains 1 treatment and 2 controls. Note that the 20 controls included in matched sets need to be different.</p>
<p><strong>How many controls?</strong></p>
<p>Let <span class="math inline">\(m\)</span> be the number of controls to be matched for one treatment. In <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-031219-041058">Paul Rosenbaum’s 2019 review on matching</a>:</p>
<blockquote>
<p>“Under a simple, familiar, conventional Gaussian model for matched sets, the variance of the estimator is proportional to 1+1/m, where the omitted constant of proportionality does not depend on m, but depends on all sorts of other things: the sample size, the variance of errors, and so on (see, for instance, Rosenbaum 2010, section 8.7)”</p>
</blockquote>
<p>For example, <span class="math inline">\(m=1\)</span> means paired matching, then <span class="math inline">\(1+\frac{1}{m} = 2\)</span>. If <span class="math inline">\(m = +\infty\)</span>, then <span class="math inline">\(1+\frac{1}{m} = 1\)</span>. Based on Rosenbaum’s comment, <span class="math inline">\(1+\frac{1}{m}\)</span> represents the variability. By adding controls from 1 to <span class="math inline">\(+\infty\)</span>, the sampling uncertainty from controls reduces but the uncertainty from treatment is unchanged. For example when <span class="math inline">\(m=10\)</span>, <span class="math inline">\(1+\frac{1}{m} = 1.1\)</span> so almost all the uncertainty come from treatment group. In practice, <span class="math inline">\(m = 3 \sim 5\)</span>.</p>
<p><strong>Pros and cons:</strong></p>
<ul>
<li>Pros: Efficiency. More samples are included inside matched sets.</li>
<li>Cons: Balance becomes terrible.</li>
</ul>
</div>
<div id="full-matching" class="section level2">
<h2><span class="header-section-number">2.6</span> Full Matching</h2>
<p>In full matching, we can accept one treated matched with multiple controls and many treated matched with one control.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sensitivity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-matching.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
