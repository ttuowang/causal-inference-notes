[
["index.html", "Causal Inference Chapter 1 Prerequisites", " Causal Inference Tuo Wang 2019-10-09 Chapter 1 Prerequisites This is the notes of Causal Inference. Most of the materials come from STAT 992 in UW-Madison instructed by Prof. Kang and from several textbooks in causal inference. Recommended textbook: Paul Rosenbaum. “Design of OBservational Studies” (2010) Miguel Hernan &amp; James Robins. “Causal Inference” (2019) Guido Imbens and Don Rubin. “Causal Inference for Statistics, Social, and Biomedical Sciences” (2015) "],
["matching.html", "Chapter 2 Matching 2.1 What is matching? 2.2 Exact Matching 2.3 Propensity Score Matching 2.4 Multivariate Caliper Matching 2.5 Matching with Multiple controls 2.6 Full Matching", " Chapter 2 Matching This lecture note will cover the following topics: Exact Matching Propensity Score Matching Multivariate Caliper Matching Matching with Multiple Controls Full Matching 2.1 What is matching? In observational study, absent random assignment, treated and control individuals may differ in terms of covariates, so direct comparison of the outcomes of treated individuals and controls may compare individuals who are not comparable - that is, a direct comparison may be biased as an estimate of the effect caused by the treatment. Pros: Simple and intuitive Blinding to outcome info Diagnostic for overlap is easy Cons: Theory is difficult It requires a lot of practice. 2.2 Exact Matching Idea: Pair people with identical \\(X\\)s 2.3 Propensity Score Matching Quick Review on Propensity Score: Propensity score is the conditional probability of exposure to treatment given the observed covariates, \\(e(\\mathbf{x}) = \\mathbb{Pr}(A = 1|\\mathbf{x})\\), where \\(\\mathbf{x}\\) is the covaraites and \\(A\\) is the intervention variable. The propensity score is unkown, but it can be estimated from the data. For example, we can use the logistic regression: \\[\\log\\{\\frac{e(\\mathbf{x_i})}{1 - e(\\mathbf{x_i})}\\} = \\beta_0 + \\mathbf{x_i}^T\\bf{\\beta}\\] Idea: Pair people with similar \\(e(X)\\). We know \\(X \\perp A | e(X)\\). Calculate the propensity score for all subjects. Define the distance between subject i in the treatment group and subject j in the control group as: \\[d(\\mathbf{x_i}, \\mathbf{x_j}) = \\big|\\text{logit}(e(\\mathbf{x_i})) - \\text{logit}(e(\\mathbf{x_i}))\\big|\\] 2.4 Multivariate Caliper Matching Why we need multivariate caliper matching? In lecture 6, we learned matching with propensity score, which tends to balance all of the covariates used to build that score, but two individuals with the same propensity score may differ in important ways. Also, propensity score matching is a single covariate matching, which ignore the interaction between different covarites. 2.4.1 (a) Mahalanobis Distances Let \\(\\mathbf{x}\\) be the covariates random vector. Let \\(\\hat{\\Sigma}\\) be the sample covariance matrix of \\(\\mathbf{x}\\). Then the estimated Mahalanobis distance between subject \\(i\\) and \\(j\\), who has covarates \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\) repectively, is defined as: \\[d(\\mathbf{x_i}, \\mathbf{x_j}) = (\\mathbf{x_i} - \\mathbf{x_j})^T\\hat{\\Sigma}^{-1}(\\mathbf{x_i} - \\mathbf{x_j})\\] See the following comments on Mahalanobis distance from Rosenbaum’s book, Design of Observational Studies: “The Mahalanobis distance was originally developed for use with multivariate Nomal data, and for data of that type it works fine. With data that are not Normal, the Mahalanobis distance can exhibit some rather odd behavior. If one covariate contains extreme outliers of has a long-tailed distribution, its standard deviation will be inflated, and the Mahalanobis distance will tend to ignore that covariate in matching.” A simple alternative to the Mahalanobis distance is the ‘rank-based Mahalanobis distance’ which replaces each of the covariates by its ranks. Check out Design of Observational Studies, Chapter 8.3 for details. 2.4.2 (b) Penalize large distance The idea is that two individuals can be close on the propensity score to a degree, once this degree is achieved, covariates \\(\\mathbf{x}\\) may affect the distance. Define \\(w\\) as the caliper width. With \\(w\\), if two individuals have propensity scores that differ more than \\(w\\), we will add a penalty to the ahalanobis distance between subject \\(i\\) and \\(j\\). Explicitly, \\[ d_{new}(\\mathbf{x_i}, \\mathbf{x_j})= \\begin{cases} d(\\mathbf{x_i}, \\mathbf{x_j}) + p\\times \\big|\\text{logit}(e(\\mathbf{x_i}))-\\text{logit}(e(\\mathbf{x_j}))\\big|,&amp; \\text{if } \\big|\\text{logit}(e(\\mathbf{x_i}))-\\text{logit}(e(\\mathbf{x_j}))\\big| \\geq w\\\\ d(\\mathbf{x_i}, \\mathbf{x_j}), &amp; \\text{if } \\big|\\text{logit}(e(\\mathbf{x_i}))-\\text{logit}(e(\\mathbf{x_j}))\\big| &lt; w \\end{cases}\\] where \\(e(\\mathbf{x})\\) is the propensity score of covariates \\(\\mathbf{x}\\), \\(d(\\mathbf{x_i}, \\mathbf{x_j})\\) is the Mahalanobis distance between individual \\(i\\) and \\(j\\) and \\(p\\) is the penalty term. In practical, we may want to care about the following things: What’s the value of \\(p\\)? Usually \\(p = 1000\\). In Paul Rosenbaum’s 2019 review on matching: he used \\(d_{new}(\\mathbf{x_i}, \\mathbf{x_j}) = +\\infty\\) when \\(\\big|\\text{logit}(e(\\mathbf{x_i}))-\\text{logit}(e(\\mathbf{x_j}))\\big| \\geq w\\). What’s the value of \\(w\\)? People use \\(w = 0.5 \\times SD(\\text{logit}(e(\\mathbf{x})))\\) or \\(w = 0.2 \\times SD(\\text{logit}(e(\\mathbf{x})))\\) in practice. Comments: Use of the Mahalanobis distance inside propensity score calipers will balance covariates and also pair similar individuals. Also, using both Mahalanobis distance and propensity score calipers adds a protection against the failure of a single matching technique. 2.5 Matching with Multiple controls In previous matching algorithms, we only match one treatment with one control. In matching with multiple controls, each treatment is matched to at least one control. For example if we match 1 treatment with 2 controls, assume we have 10 subjects in treatment group, we will end up with 10 matched sets, which each contains 1 treatment and 2 controls. Note that the 20 controls included in matched sets need to be different. How many controls? Let \\(m\\) be the number of controls to be matched for one treatment. In Paul Rosenbaum’s 2019 review on matching: “Under a simple, familiar, conventional Gaussian model for matched sets, the variance of the estimator is proportional to 1+1/m, where the omitted constant of proportionality does not depend on m, but depends on all sorts of other things: the sample size, the variance of errors, and so on (see, for instance, Rosenbaum 2010, section 8.7)” For example, \\(m=1\\) means paired matching, then \\(1+\\frac{1}{m} = 2\\). If \\(m = +\\infty\\), then \\(1+\\frac{1}{m} = 1\\). Based on Rosenbaum’s comment, \\(1+\\frac{1}{m}\\) represents the variability. By adding controls from 1 to \\(+\\infty\\), the sampling uncertainty from controls reduces but the uncertainty from treatment is unchanged. For example when \\(m=10\\), \\(1+\\frac{1}{m} = 1.1\\) so almost all the uncertainty come from treatment group. In practice, \\(m = 3 \\sim 5\\). Pros and cons: Pros: Efficiency. More samples are included inside matched sets. Cons: Balance becomes terrible. 2.6 Full Matching In full matching, we can accept one treated matched with multiple controls and many treated matched with one control. "],
["sensitivity.html", "Chapter 3 Sensitivity Analysis", " Chapter 3 Sensitivity Analysis "]
]
