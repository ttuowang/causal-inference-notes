# causal Inference with Models {#models}

Throughout this chapter, we have the following set up :

- $Y_i$, $A_i$, $X_i$ i.i.d $\sim$ $F$

and assumptions:

- 1. $Y_{i}(1)A_i + Y_{i}(0)(1-A_i) = Y_i$
- 2. $Y_{i}(1),Y_{i}(0) \perp A_i | X_i$
- 3. $0 < \mathbb{P}(A_i = 1 | X_i) < 1$

**Goal:** Estimate $\tau = \mathbb{E}[Y_i(1) - Y_i(0)]$

We introduce four methods:

- IPW: Inverse probability weighting 
- Outcome Regression /G-formula
- Double Robust estimation
- Machine learning

**Under assumption 1-3:**

$$\tau = \mathbb{E} \bigg[\frac{Y_iA_i}{e(X_i)} - \frac{Y_i(1-A_i)}{1-e(X_i)}\bigg]\,\,\,\,\,\,(\text{inverse probability weighting})$$

$$\tau = \mathbb{E}_X \big\{ \mathbb{E} \big[ Y_i|A_i=1,X_i\big] - \mathbb{E} 
\big[ Y_i|A_i=0,X_i\big]\big\} = \mathbb{E}_X\big[f_1(X_i) - f_0(X_i)\big]
\,\,\,\,\,\,(\text{Outcome regression})$$

**Prove:** $\mathbb{E}[Y_i(1)] = \mathbb{E}[\frac{Y_iA_i}{e(X_i)}]$ and $\mathbb{E}[Y_i(0)] = \mathbb{E}[\frac{Y_i(1-A_i)}{1-e(X_i)}]$

**proof :**

By assumption 3, $\mathbb{E}\big[\frac{Y_iA_i}{e(X_i)}\big]$ is well defined. Thus,

\begin{align*}

\mathbb{E}\big[\frac{Y_i(1-A_i)}{e(X_i)}\big] &=\mathbb{E}_X \big\{
\mathbb{E}\big[\frac{Y_i(1-A_i)}{e(X_i)} | X_i\big] \big\} \\
&= \mathbb{E}_X \big\{ \frac{1}{e(X_i)} \mathbb{E} \big[ Y_iA_i | X_i\big] \big\} \\
&= \mathbb{E}_X \big\{ \frac{1}{e(X_i)} \mathbb{E} \big[ (Y_{i}(1)A_i + Y_{i}(0)(1-A_i))A_i | X_i\big] \big\} \\
&= \mathbb{E}_X \big\{ \frac{1}{e(X_i)} \mathbb{E} \big[ Y_i(1)A_i | X_i\big] \big\} \\
&= \mathbb{E}_X \big\{ \frac{1}{e(X_i)} \mathbb{E} \big[ Y_i(1) | X_i\big] \mathbb{E} \big[ A_i | X_i\big] \big\} \\
&= \mathbb{E}_X \big\{ \mathbb{E} \big[ Y_i(1) | X_i\big] \big\} \\
&= \mathbb{E}[Y_i(1)]
\end{align*}

We can prove $\mathbb{E}[Y_i(0)] = \mathbb{E}[\frac{Y_i(1-A_i)}{1-e(X_i)}]$ by using the same procedure.

**Prove:** $\mathbb{E}[Y_i(1)]=\mathbb{E}_X\{ \mathbb{E}[Y_i | A_i=1, X_i] \}$ and $\mathbb{E}[Y_i(0)]=\mathbb{E}_X\{ \mathbb{E}[Y_i | A_i=0, X_i] \}$

**proof:** 
\begin{align*}
\mathbb{E}_X\{ \mathbb{E}[Y_i | A_i=1, X_i] \} &= \mathbb{E}_X\{ \mathbb{E}[Y_{i}(1)A_i + Y_{i}(0)(1-A_i) | A_i=1, X_i] \} \\
&= \mathbb{E}_X\{ \mathbb{E}[Y_{i}(1) | A_i=1, X_i] \} \\
&= \mathbb{E}_X\{ \mathbb{E}[Y_{i}(1) |X_i] \} \\
&= \mathbb{E}[Y_{i}(1)]
\end{align*}

We can prove $\mathbb{E}[Y_i(0)]=\mathbb{E}_X\{ \mathbb{E}[Y_i | A_i=0, X_i] \}$ by using the same procedure.

## IPW estimator

**Idea:** replace $\mathbb{E}[.]$ with sample means.

$$\hat{\Large\tau}_{IPW} = \frac{1}{n} \sum_{i=1}^{n}\frac{Y_iA_i}{e(X_i)} - \frac{1}{n} \sum_{i=1}^{n}\frac{Y_i(1-A_i)}{1-e(X_i)}$$
To use $\hat{\Large\tau}_{IPW}$, you need to know $e(X_i) = \mathbb{P}(A_i=1 |X_i)$. 

From **Law of Large Number**:

$$\frac{1}{n} \sum_{i=1}^{n}\frac{Y_iA_i}{e(X_i)} \rightarrow \mathbb{E} \big[\frac{Y_iA_i}{e(X_i)} \big]$$
$$\frac{1}{n} \sum_{i=1}^{n}\frac{Y_i(1-A_i)}{1-e(X_i)} \rightarrow \mathbb{E} \big[\frac{Y_i(1-A_i)}{1-e(X_i)} \big]$$
Thus, 
$$\hat{\Large\tau}_{IPW} \rightarrow \Large\tau$$
Rewrite,
$$\hat{\Large\tau}_{IPW} = \frac{1}{n} \sum_{i=1}^{n}\bigg [\frac{Y_iA_i}{e(X_i)} - \frac{Y_i(1-A_i)}{1-e(X_i)} \bigg ]$$
Further, from **Central Limit Theorem:** we have:

$$\sqrt{n}(\hat{\Large\tau}_{IPW} - {\Large \tau}) \rightarrow N(0, \text{var}(\frac{Y_iA_i}{e(X_i)} - \frac{Y_i(1-A_i)}{1-e(X_i)}))$$

## Outcome Regression

$$\hat{\Large\tau}_{OR} = \frac{1}{n}\sum_{i=1}^{n} [f_1(X_i) - f_0(X_i)],$$
where $f_1(X_i) = \mathbb{E} \big[ Y_i|A_i=1,X_i\big]$ and $f_0(X_i) = \mathbb{E} \big[ Y_i|A_i=0,X_i\big]$.

To use $\hat{\Large\tau}_{OR}$, you need to know $f_1(X_i)$ and $f_0(X_i)$.

From **Law of Large Number:**

$$\hat{\Large\tau}_{OR}= \frac{1}{n}\sum_{i=1}^{n} [f_1(X_i) - f_0(X_i)] \rightarrow \mathbb{E}[f_1(X_i) - f_0(X_i)] = {\Large\tau}$$
And further from **Central Limit Theorem:**

$$\sqrt{n}(\hat{\Large\tau}_{OR} - {\Large \tau}) \rightarrow N(0, \text{var}(f_1(X_i) - f_0(X_i)))$$

## Doubly robust estimator

$$\hat{\Large\tau}_{DR} = \frac{1}{n}\sum_{i=1}^{n} \bigg[\frac{(Y_i - f_1(X_i))A_i}{e(X_i)} + f_1(X_i)\bigg] - \frac{1}{n}\sum_{i=1}^{n} \bigg[\frac{(Y_i - f_0(X_i))(1-A_i)}{1-e(X_i)} + f_0(X_i)\bigg]$$

Suppose $f_1(X_i)$, $f_0(X_i)$, $e(X_i)$ are correct, we can show $\hat{\Large\tau}_{DR} \rightarrow {\Large\tau}$


**proof:**

\begin{align*}
\mathbb{E} \bigg[  \frac{(Y_i - f_1(X_i))A_i}{e(X_i)}\bigg] &= \mathbb{E}_X \bigg\{\mathbb{E} \bigg[  \frac{(Y_i - f_1(X_i))A_i}{e(X_i)}|X_i\bigg] \bigg\} \\
&= \mathbb{E}_X \bigg\{ \frac{1}{e(X_i)} (\mathbb{E}[Y_iA_i|X_i] - \mathbb{E}[f_1(X_i)A_i|X_i])  \bigg\} \\
&= \mathbb{E}_X \bigg\{ \frac{1}{e(X_i)} (\mathbb{E}[Y_i|A_1 = 1, X_i] \mathbb{P}(A_i=1|X_i) - f_1(X_i)\mathbb{E}[A_i|X_i])  \bigg\} \\
&=0
\end{align*}

The same holds for the other part. Then,
$$\hat{\Large\tau}_{DR} \rightarrow \mathbb{E}[f_1(X_i)]-\mathbb{E}[f_0(X_i)]\rightarrow {\Large\tau}$$

Suppose the estimate propensity score is different from the true propensity score, i.e. $\hat{e}(X) \neq e(X)$. Assume $f_1$ and $f_0$ are known.

$$\hat{\Large\tau}_{DR,\hat{e}} = \frac{1}{n}\sum_{i=1}^{n} \bigg[\frac{(Y_i - f_1(X_i))A_i}{\hat e(X_i)} + f_1(X_i)\bigg] - \frac{1}{n}\sum_{i=1}^{n} \bigg[\frac{(Y_i - f_0(X_i))(1-A_i)}{1-\hat e(X_i)} + f_0(X_i)\bigg]$$

We still have 

$$\hat{\Large\tau}_{DR,\hat{e}} \rightarrow {\Large\tau}$$

Suppose $\hat f_1 \neq f_1$ and $\hat f_0 \neq f_0$ but $e$ is correct.

\begin{align*}
\hat{\Large\tau}_{DR,\hat{f}_1,\hat{f}_0} &= \frac{1}{n}\sum_{i=1}^{n} \bigg[\frac{(Y_i - \hat f_1(X_i))A_i}{ e(X_i)} + \hat f_1(X_i)\bigg] - \frac{1}{n}\sum_{i=1}^{n} \bigg[\frac{(Y_i - \hat f_0(X_i))(1-A_i)}{1-e(X_i)} + \hat f_0(X_i)\bigg] \\
&= \frac{1}{n} \sum_{i=1}^{n} \bigg(\frac{Y_iA_i}{e(X_i)} - \frac{Y_i(1-A_i)}{1-e(X_i)}\bigg) + \frac{1}{n} \sum_{i=1}^{n} \bigg(\hat f_1(X_i) - \frac{\hat f_1(X_i)A_i}{e(X_i)} - \hat f_0(X_i) - \frac{\hat f_0(X_i)A_i}{1-e(X_i)}\bigg)\\ 
&= \hat{\Large\tau}_{IPW} + \frac{1}{n} \sum_{i=1}^{n} \bigg(\hat f_1(X_i) - \frac{\hat f_1(X_i)A_i}{e(X_i)} - \hat f_0(X_i) - \frac{\hat f_0(X_i)A_i}{1-e(X_i)}\bigg)
\end{align*}

We know the first part $\hat{\Large\tau}_{IPW} \rightarrow {\Large\tau}$ and it is easy to show the latter part goes to zero. Thus,

$$\hat{\Large\tau}_{DR,\hat{f}_1,\hat{f}_0} \rightarrow {\Large\tau}$$

Suppose replace $f_1$,$f_0$,$e$ with estimate function $\hat f_1$,$\hat f_0$,$\hat e$. If $\hat f_1$,$\hat f_0$,$\hat e$ are all l2 converge to $f_1$,$f_0$,$e$, then 

$$\hat{\Large\tau}_{DR,\hat{e},\hat{f}_1,\hat{f}_0} \rightarrow {\Large\tau}$$


Roses are $\color{red}{\text{beautiful red}}$, 
violets are $\color{blue}{\text{lovely blue}}$.
















